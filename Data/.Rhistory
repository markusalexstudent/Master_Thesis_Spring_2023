mutate(url_complete = paste0("https://newsweb.oslobors.no", message))
View(df)
# Create list to store dataframes in:
df_list <- list()
message_url_list <- list()
urls_list <- urls_list[1:200]
# Extract meta data for all links in url list:
for(i in seq_along(urls_list)) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(urls_list[[i]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df_list[[i]] <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_url_list[[i]] <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
#print(message_url_list[[i]])
progress(i, length(urls_list)) # Track progress of loop
}
# Remove the NULL elements from the list
df_list <- Filter(function(x) nrow(x) > 0, df_list)
# Merge dataframes within each list:
df <- bind_rows(df_list)
message_urls <- bind_rows(message_url_list)
# Clean data:
message_urls <- data.frame(message_urls[grep("message", message_urls$.),])
colnames(message_urls) <- "message"
# Merge dataframes:
df$index = seq(1:nrow(df))
message_urls$index = seq(1:nrow(df))
df <- merge(df, message_urls, by = "index")
# Create an url column:
df <- df %>%
mutate(url_complete = paste0("https://newsweb.oslobors.no", message))
View(df)
View(df_list)
# Close remDr:
remDr$close()
gc()
system("taskkill /im Docker.exe /f", intern=FALSE, ignore.stdout=FALSE)
R.version()
version
################################################################################
#                            Plots and tables
################################################################################
# Preamble, setting working directory
rm(list = ls())
function (x, df1, df2, ncp, log = FALSE)
setwd("C:/Users/marku/OneDrive/Skrivebord/MASTEROPPGAVE/Master_thesis/GITHUB REPOSITORY/Master_Thesis_Spring_2023/Data/")
options(scipen = 999)
## packages
library(xml2)
library(tidyverse)
library(tidytext)                         # Clean text
library(readxl)                           # Load excel files
library(lubridate)                        # Manipulate dates
library(dplyr)
library(kableExtra)
library(multidplyr)                       # Dplyr with parallel
library(openxlsx)
library(remotes)
library(stringr)
library(slam)
library(SentimentAnalysis)
library(TTR)
library(plm)                              # Panel data
library(wordcloud)
library(wordcloud2)
library(quanteda)
library(textmineR)
library(DescTools)
library(quanteda)
library(rrtable)
#-------------------------------------------------------------------------------
# Plot event window
#-------------------------------------------------------------------------------
meta <- readRDS("fromR/metaAdj_v4.rds")
colnames(meta)
# Calculate mean returns around an announcement:
eventWindow <- meta %>%
summarise(
median_lag5 = median(abs(lag_retMkt5), na.rm = T),
median_lag4 = median(abs(lag_retMkt), na.rm = T),
median_lag3 = median(abs(lag_retMkt3), na.rm = T),
median_lag2 = median(abs(lag_retMkt2), na.rm = T),
median_lag = median(abs(lag_retMkt), na.rm = T),
median = median(abs(retMkt), na.rm = T),
median_lead = median(abs(lead_retMkt), na.rm = T),
median_lead_2 = median(abs(lead_retMkt2), na.rm = T),
median_lead_3 = median(abs(lead_retMkt3), na.rm = T),
median_lead_4 = median(abs(lead_retMkt4), na.rm = T),
median_lead_5 = median(abs(lead_retMkt5), na.rm = T),
ret_lag5 = mean(abs(lag_retMkt5), na.rm = T),
ret_lag4 = mean(abs(lag_retMkt4), na.rm = T),
ret_lag3 = mean(abs(lag_retMkt3), na.rm = T),
ret_lag2 = mean(abs(lag_retMkt2), na.rm = T),
ret_lag1 = mean(abs(lag_retMkt), na.rm = T),
ret1 = mean(abs(retMkt), na.rm = T),
lead_ret1 = mean(abs(lead_retMkt), na.rm = T),
lead_ret2 = mean(abs(lead_retMkt2), na.rm = T),
lead_ret3 = mean(abs(lead_retMkt3), na.rm = T),
lead_ret4 = mean(abs(lead_retMkt4), na.rm = T),
lead_ret5 = mean(abs(lead_retMkt5), na.rm = T)) %>%
t() %>%
as.data.frame() %>%
dplyr::rename(return = V1)
eventWindow <- data.frame(mean = eventWindow[12:22,1],
median = eventWindow[1:11,1])
eventWindow$time = c(-5,-4,-3,-2,-1,0,1,2,3,4,5)
p1 <- eventWindow %>%
pivot_longer(1:2, names_to = "measure") %>%
ggplot(aes(x = time, y = value, color = measure)) +
geom_point(data = . %>% filter(measure == "mean"),
size = 5, color = "blue", shape = 6) +
geom_line(data = . %>% filter(measure == "mean"),
color = "blue", linetype = "dotted", linewidth = 1, alpha = 0.7) +
geom_point(data = . %>% filter(measure == "median"),
size = 5, color = "green", shape = 4) +
geom_line(data = . %>% filter(measure == "median"),
color = "green", linetype = "dashed", linewidth = 1, alpha = 0.7) +
geom_vline(xintercept = 0, color = "darkgray", linetype = "dotted") +
ylab("Average/Median Absolute Excess Return") +
xlab("Time") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold")) +
labs(color = "Measure")
p1
ggsave("tables_and_plots/plots/p1.pdf", plot = p1, width = 7, height = 5)
################################################################################
#                            Plots and tables
################################################################################
# Preamble, setting working directory
rm(list = ls())
setwd("C:/Users/marku/OneDrive/Skrivebord/MASTEROPPGAVE/Master_thesis/GITHUB REPOSITORY/Master_Thesis_Spring_2023/Data/")
options(scipen = 999)
## packages
library(xml2)
library(tidyverse)
library(tidytext)                         # Clean text
library(readxl)                           # Load excel files
library(lubridate)                        # Manipulate dates
library(dplyr)
library(kableExtra)
library(multidplyr)                       # Dplyr with parallel
library(openxlsx)
library(remotes)
library(stringr)
library(slam)
library(SentimentAnalysis)
library(TTR)
library(plm)                              # Panel data
library(wordcloud)
library(wordcloud2)
library(quanteda)
library(textmineR)
library(DescTools)
library(quanteda)
library(rrtable)
#-------------------------------------------------------------------------------
# Plot event window
#-------------------------------------------------------------------------------
meta <- readRDS("fromR/metaAdj_v4.rds")
colnames(meta)
# Calculate mean returns around an announcement:
eventWindow <- meta %>%
summarise(
median_lag5 = median(abs(lag_retMkt5), na.rm = T),
median_lag4 = median(abs(lag_retMkt), na.rm = T),
median_lag3 = median(abs(lag_retMkt3), na.rm = T),
median_lag2 = median(abs(lag_retMkt2), na.rm = T),
median_lag = median(abs(lag_retMkt), na.rm = T),
median = median(abs(retMkt), na.rm = T),
median_lead = median(abs(lead_retMkt), na.rm = T),
median_lead_2 = median(abs(lead_retMkt2), na.rm = T),
median_lead_3 = median(abs(lead_retMkt3), na.rm = T),
median_lead_4 = median(abs(lead_retMkt4), na.rm = T),
median_lead_5 = median(abs(lead_retMkt5), na.rm = T),
ret_lag5 = mean(abs(lag_retMkt5), na.rm = T),
ret_lag4 = mean(abs(lag_retMkt4), na.rm = T),
ret_lag3 = mean(abs(lag_retMkt3), na.rm = T),
ret_lag2 = mean(abs(lag_retMkt2), na.rm = T),
ret_lag1 = mean(abs(lag_retMkt), na.rm = T),
ret1 = mean(abs(retMkt), na.rm = T),
lead_ret1 = mean(abs(lead_retMkt), na.rm = T),
lead_ret2 = mean(abs(lead_retMkt2), na.rm = T),
lead_ret3 = mean(abs(lead_retMkt3), na.rm = T),
lead_ret4 = mean(abs(lead_retMkt4), na.rm = T),
lead_ret5 = mean(abs(lead_retMkt5), na.rm = T)) %>%
t() %>%
as.data.frame() %>%
dplyr::rename(return = V1)
eventWindow <- data.frame(mean = eventWindow[12:22,1],
median = eventWindow[1:11,1])
eventWindow$time = c(-5,-4,-3,-2,-1,0,1,2,3,4,5)
p1 <- eventWindow %>%
pivot_longer(1:2, names_to = "measure") %>%
ggplot(aes(x = time, y = value, color = measure)) +
geom_point(data = . %>% filter(measure == "mean"),
size = 5, color = "blue", shape = 6) +
geom_line(data = . %>% filter(measure == "mean"),
color = "blue", linetype = "dotted", linewidth = 1, alpha = 0.7) +
geom_point(data = . %>% filter(measure == "median"),
size = 5, color = "green", shape = 4) +
geom_line(data = . %>% filter(measure == "median"),
color = "green", linetype = "dashed", linewidth = 1, alpha = 0.7) +
geom_vline(xintercept = 0, color = "darkgray", linetype = "dotted") +
ylab("Average/Median Absolute Excess Return") +
xlab("Time") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold")) +
labs(color = "Measure")
p1
ggsave("tables_and_plots/plots/p1.pdf", plot = p1, width = 7, height = 5)
rm(eventWindow, p1)
#-------------------------------------------------------------------------------
# DF data
#-------------------------------------------------------------------------------
df <- readRDS("fromR/df_meta.rds")
#-------------------------------------------------------------------------------
# Frequency graphs
#-------------------------------------------------------------------------------
df <- readRDS("fromR/df_meta.rds")
df <- df %>%
mutate(year = as.numeric(substr(time, 7, 10)))
# Frequency of year:
p <- meta %>%
group_by(year) %>%
summarise(nr_articles = n()) %>%
filter(year < 2023) %>%
ggplot(aes(x=year, y = nr_articles)) +
geom_bar(stat="identity", fill="blue", width=.8) +
xlab("\nYear") +
ylab("Number of press releases\n") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold")) +
scale_x_continuous(breaks = seq(min(meta$year), max(meta$year), 1), minor_breaks = NULL)
p
ggsave("tables_and_plots/plots/p10.pdf", plot = p, width = 7, height = 5)
# Frequency by firm:
p <- meta %>%
group_by(year) %>%
summarise(uniqueTickers = n_distinct(ticker)) %>%
filter(year < 2023) %>%
ggplot(aes(x=year, y = uniqueTickers)) +
geom_bar(stat="identity", fill="blue", width=.8) +
xlab("\nYear") +
ylab("Number of press releases\n") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold")) +
scale_x_continuous(breaks = seq(min(meta$year), max(meta$year), 1), minor_breaks = NULL)
p
unique(df$market)
p <- df %>%
filter(year < 2023,
lang == "en") %>%
group_by(year) %>%
summarise(uniqueTickers = n_distinct(ticker),
uniquePress = n(),
pressPerTicker = uniquePress/uniqueTickers) %>%
ggplot(aes(x=year, y = uniqueTickers)) +
geom_bar(stat="identity", fill="blue", width=.8) +
xlab("\nYear") +
ylab("Number of press releases\n") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold")) +
scale_x_continuous(breaks = seq(min(meta$year), max(meta$year), 1), minor_breaks = NULL)
p
p <- df %>%
filter(year < 2023,
lang == "en") %>%
group_by(year) %>%
summarise(uniqueTickers = n_distinct(ticker),
uniquePress = n(),
pressPerTicker = uniquePress/uniqueTickers)
p
p <- df %>%
filter(year < 2023,
lang == "en") %>%
group_by(year) %>%
summarise(uniqueTickers = n_distinct(ticker),
uniquePress = n(),
pressPerTicker = uniquePress/uniqueTickers) %>%
ggplot(aes(x=uniqueTickers, y = uniquePress)) +
geom_point() +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold")) +
scale_x_continuous(breaks = seq(min(meta$year), max(meta$year), 1), minor_breaks = NULL)
p
p <- df %>%
filter(year < 2023,
lang == "en") %>%
group_by(year) %>%
summarise(uniqueTickers = n_distinct(ticker),
uniquePress = n(),
pressPerTicker = uniquePress/uniqueTickers) %>%
ggplot(aes(x=uniqueTickers, y = uniquePress)) +
geom_point() +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold"))
p
p <- df %>%
filter(year < 2023,
lang == "en") %>%
group_by(year) %>%
summarise(uniqueTickers = n_distinct(ticker),
uniquePress = n(),
pressPerTicker = uniquePress/uniqueTickers) %>%
ggplot(aes(x=uniqueTickers, y = uniquePress, color = year)) +
geom_point() +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold"))
p
p <- meta %>%
filter(year < 2023,
lang == "en") %>%
group_by(year) %>%
summarise(uniqueTickers = n_distinct(ticker),
uniquePress = n(),
pressPerTicker = uniquePress/uniqueTickers) %>%
ggplot(aes(x=uniqueTickers, y = uniquePress, color = year)) +
geom_point() +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold"))
p
View(meta)
View(meta)
p <- meta %>%
ggplot(aes(x = words, y = ret)) +
geom_point()
p
View(p)
View(df)
View(meta)
p <- meta %>%
ggplot(aes(x = volume, y = ret)) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = volume, y = abs(ret))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = log(volume), y = abs(ret))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = log(volume), y = log(abs(ret)))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = words, y = log(abs(ret)))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = words, y = abs(ret))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = words, y = log(abs(ret)))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = mcap, y = log(abs(ret)))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = log(mcap), y = abs(ret))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = log(mcap), y = abs(ret))) +
ylim(1) %>%
geom_point()
p
meta$ret <- Winsorize(meta$ret, probs = c(0.01, 0.99))
p <- meta %>%
ggplot(aes(x = log(mcap), y = abs(ret))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = log(turnover), y = abs(ret))) +
geom_point()
p
p <- meta %>%
ggplot(aes(x = turnover, y = abs(ret))) +
geom_point()
p
p <- meta %>%
filter(year < 2023,
lang == "en") %>%
group_by(year) %>%
summarise(uniqueTickers = n_distinct(ticker),
uniquePress = n(),
pressPerTicker = uniquePress/uniqueTickers) %>%
ggplot(aes(x=uniqueTickers, y = uniquePress, color = year)) +
geom_point() +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold"))
p
citation(dplyr)
citation("dplyr")
citation("textir")
citation("multidplyr")
# Packages used
library(eikonapir)                        # Refinitiv Eikon API for R
library(RSelenium)                        # Docker and Selenium
library(rvest)                            # Webscraping
library(svMisc)                           # Progress tracker
library(tidyverse)                        # Dplyr etc.
library(tidytext)                         # Clean text
library(textmineR)                        # DTM and TF-IDF filtering
library(tm)                               # Textmining (incl. stopwords)
library(textcat)                          # Detect language
library(cld2)                             # Detect language (superior to textcat --> https://stackoverflow.com/questions/8078604/detect-text-language-in-r)
library(parallel)                         # Parallel computing
library(readxl)                           # Load excel files
library(lubridate)                        # Manipulate dates
library(PerformanceAnalytics)             # Calculate returns etc.
library(quantmod)                         # Stopwords, textual analysis etc.
library(quanteda)                         # Text analysis
library(data.table)                       # Work with tables
library(textir)                           # MNIR ML Model
library(rrapply)                          # recursively apply a function to elements of a nested list based on a general condition function
library(kableExtra)                       # Create latex tables
library(multidplyr)                       # Dplyr with parallel
library(openxlsx)                         # Save and read .xlsx
library(bizdays)                          # Business days
library(reticulate)                       # Run Python in R
library(remotes)                          # Install remote repos (e.g., github)
library(stringr)                          # Use wrappers
library(slam)                             # Sparse Lightweight Arrays and Matrices --> convert DTM to sparse matrix
library(SentimentAnalysis)                # LM dictionary
library(felm)                             # Panel data regressions
packages()
library()
print(sessionInfo())
install.packages("NCmisc")
NCmsic::list.functions.in.file(filename, alphabetic = TRUE)
library(NCmisc)
list.functions.in.file(filename, alphabetic = TRUE)
list.functions.in.file("C:/Users/marku/OneDrive/Skrivebord/MASTEROPPGAVE/Master_thesis/GITHUB REPOSITORY/Master_Thesis_Spring_2023/script", alphabetic = TRUE)
setwd("C:/Users/marku/OneDrive/Skrivebord/MASTEROPPGAVE/Master_thesis/GITHUB REPOSITORY/Master_Thesis_Spring_2023/script")
list.functions.in.file(getwd(), alphabetic = TRUE)
print(sessionInfo())
print(sessionInfo())[2]
t <- print(sessionInfo())
View(t)
t$otherPkgs
t <- t$otherPkgs
t
View(t)
# Extract the package names from the loaded packages
package_list <- session_info$loadedOnly
session_info
packages <- c("eikonapir", "slam")
setwd("C:/Users/marku/OneDrive/Skrivebord/MASTEROPPGAVE/Master_thesis/GITHUB REPOSITORY/Master_Thesis_Spring_2023/Data/")
packages <- c("eikonapir", "slam")
# Create a file to store the BibTeX entries
file_path <- "r_packages.bib"
file <- file(file_path, "w")
file
file_path
# Loop through the packages and generate BibTeX entries
for (pkg in packages) {
citation <- citation(pkg)
writeLines(citation$text, file)
writeLines("", file)  # Add an empty line between entries
}
citation("eikonapir")
sessionInfo()
library(report)
install.packages("library(report)")
install.packages("report")
install.packages("report")
library(report)
cite_packages()
