html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_url_list[[i]] <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
#print(message_url_list[[i]])
progress(i, length(urls_list)) # Track progress of loop
}
# Create list to store dataframes in:
df_list <- list()
message_url_list <- list()
# Function to extract meta data for a single URL
extract_meta_data <- function(url) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(url)
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_urls <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
# Return the dataframes
list(df, message_urls)
}
# Function to extract meta data for a single URL
extract_meta_data <- function(url) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(url)
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_urls <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
# Return the dataframes
list(df, message_urls)
progress(url, length(urls_list))
}
results_list <- lapply(urls_list, extract_meta_data)
# Function to extract meta data for a single URL
extract_meta_data <- function(url) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(url)
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_urls <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
# Return the dataframes
list(df, message_urls)
}
results_list <- lapply(urls_list, extract_meta_data)
# Create list to store dataframes in:
df_list <- list()
message_url_list <- list()
# Function to extract meta data for a single URL
extract_meta_data <- function(url) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(url)
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_urls <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
# Return the dataframes
list(df, message_urls)
progress(which(urls_list == url), length(urls_list))
}
# Extract meta data for all links in url list:
results_list <- lapply(urls_list, extract_meta_data)
# Create list to store dataframes in:
df_list <- list()
message_url_list <- list()
# Function to extract meta data for a single URL
extract_meta_data <- function(url) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(url)
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_urls <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
# Return the dataframes
list(df, message_urls)
# Track progress:
progress(which(urls_list == url), length(urls_list))
}
# Running parallel processing:
clust <- makeCluster(n.cores - 4)
library(parallel)
# Running parallel processing:
clust <- makeCluster(n.cores - 4)
# Running parallel processing:
n.cores = detectCores()
clust <- makeCluster(n.cores - 4)
clust
results_list <- parLapply(clust, urls_list, extract_meta_data)
results_list <- lapply(urls_list, extract_meta_data)
# Function to extract meta data for a single URL
extract_meta_data <- function(url) {
library(RSelenium)                        # Docker and Selenium
library(rvest)
library(dplyr)
remDr <-  RSelenium::remoteDriver(
remoteServerAddr = "localhost",
port = 4445L,
browserName = "firefox"
)
remDr$open()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(url)
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_urls <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
# Return the dataframes
list(df, message_urls)
# Track progress:
progress(which(urls_list == url), length(urls_list))
}
n.cores = detectCores()
clust <- makeCluster(n.cores - 4)
results_list <- parLapply(clust, urls_list, extract_meta_data)
stopCluster()
# Create list to store dataframes in:
df_list <- list()
message_url_list <- list()
# Function to extract meta data for a single URL
extract_meta_data <- function(url) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(url)
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_urls <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
# Return the dataframes
list(df, message_urls)
# Track progress:
progress(which(urls_list == url), length(urls_list))
}
results_list <- lapply(urls_list, extract_meta_data)
View(results_list)
results_list[[1]]
# Create list to store dataframes in:
df_list <- list()
message_url_list <- list()
urls_list()
urls_list
urls_list <- urls_list[1:200]
# Extract meta data for all links in url list:
for(i in seq_along(urls_list)) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(urls_list[[i]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df_list[[i]] <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_url_list[[i]] <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
#print(message_url_list[[i]])
progress(i, length(urls_list)) # Track progress of loop
}
# Unpack the results into separate lists
df_list <- lapply(results_list, function(x) x[[1]])
message_url_list <- lapply(results_list, function(x) x[[2]])
# Merge dataframes within each list:
df <- bind_rows(df_list)
message_urls <- bind_rows(message_url_list)
# Clean data:
message_urls <- data.frame(message_urls[grep("message", message_urls$.),])
colnames(message_urls) <- "message"
urls_list <- urls_list[1:3]
# Create list to store dataframes in:
df_list <- list()
message_url_list <- list()
urls_list <- urls_list[1:3]
# Extract meta data for all links in url list:
for(i in seq_along(urls_list)) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(urls_list[[i]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df_list[[i]] <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_url_list[[i]] <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
#print(message_url_list[[i]])
progress(i, length(urls_list)) # Track progress of loop
}
View(df_list)
urls_list
# Remove the NULL elements from the list
df_list <- Filter(Negate(is.null), df_list)
View(df_list)
# Remove the NULL elements from the list
df_list <- Filter(function(x) nrow(x) > 0, df_list)
# Merge dataframes within each list:
df <- bind_rows(df_list)
message_urls <- bind_rows(message_url_list)
# Clean data:
message_urls <- data.frame(message_urls[grep("message", message_urls$.),])
colnames(message_urls) <- "message"
# Merge dataframes:
df$index = seq(1:nrow(df))
message_urls$index = seq(1:nrow(df))
df <- merge(df, message_urls, by = "index")
# Create an url column:
df <- df %>%
mutate(url_complete = paste0("https://newsweb.oslobors.no", message))
View(df)
# Create list to store dataframes in:
df_list <- list()
message_url_list <- list()
urls_list <- urls_list[1:200]
# Extract meta data for all links in url list:
for(i in seq_along(urls_list)) {
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
remDr$navigate(urls_list[[i]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
doc <- read_html(remDr$getPageSource()[[1]])
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract df:
df_list[[i]] <- doc %>%
html_table() %>%
data.frame()
Sys.sleep(1) # This is added so that there is time to load each page in FireFox
# Extract links to text:
message_url_list[[i]] <- doc %>%
html_nodes("a") %>% html_attr('href') %>%
data.frame()
#print(message_url_list[[i]])
progress(i, length(urls_list)) # Track progress of loop
}
# Remove the NULL elements from the list
df_list <- Filter(function(x) nrow(x) > 0, df_list)
# Merge dataframes within each list:
df <- bind_rows(df_list)
message_urls <- bind_rows(message_url_list)
# Clean data:
message_urls <- data.frame(message_urls[grep("message", message_urls$.),])
colnames(message_urls) <- "message"
# Merge dataframes:
df$index = seq(1:nrow(df))
message_urls$index = seq(1:nrow(df))
df <- merge(df, message_urls, by = "index")
# Create an url column:
df <- df %>%
mutate(url_complete = paste0("https://newsweb.oslobors.no", message))
View(df)
View(df_list)
# Close remDr:
remDr$close()
gc()
system("taskkill /im Docker.exe /f", intern=FALSE, ignore.stdout=FALSE)
R.version()
version
################################################################################
#                            Plots and tables
################################################################################
# Preamble, setting working directory
rm(list = ls())
setwd("C:/Users/marku/OneDrive/Skrivebord/MASTEROPPGAVE/Master_thesis/GITHUB REPOSITORY/Master_Thesis_Spring_2023/Data/")
options(scipen = 999)
## packages
library(xml2)
library(tidyverse)
library(tidytext)                         # Clean text
library(readxl)                           # Load excel files
library(lubridate)                        # Manipulate dates
library(dplyr)
library(kableExtra)
library(multidplyr)                       # Dplyr with parallel
library(openxlsx)
library(remotes)
library(stringr)
library(slam)
library(SentimentAnalysis)
library(TTR)
library(plm)                              # Panel data
library(wordcloud)
library(wordcloud2)
library(quanteda)
library(textmineR)
library(DescTools)
library(quanteda)
library(rrtable)
#-------------------------------------------------------------------------------
# Plot event window
#-------------------------------------------------------------------------------
meta <- readRDS("fromR/metaAdj_v4.rds")
colnames(meta)
# Calculate mean returns around an announcement:
eventWindow <- meta %>%
summarise(
median_lag5 = median(abs(lag_retMkt5), na.rm = T),
median_lag4 = median(abs(lag_retMkt), na.rm = T),
median_lag3 = median(abs(lag_retMkt3), na.rm = T),
median_lag2 = median(abs(lag_retMkt2), na.rm = T),
median_lag = median(abs(lag_retMkt), na.rm = T),
median = median(abs(retMkt), na.rm = T),
median_lead = median(abs(lead_retMkt), na.rm = T),
median_lead_2 = median(abs(lead_retMkt2), na.rm = T),
median_lead_3 = median(abs(lead_retMkt3), na.rm = T),
median_lead_4 = median(abs(lead_retMkt4), na.rm = T),
median_lead_5 = median(abs(lead_retMkt5), na.rm = T),
ret_lag5 = mean(abs(lag_retMkt5), na.rm = T),
ret_lag4 = mean(abs(lag_retMkt4), na.rm = T),
ret_lag3 = mean(abs(lag_retMkt3), na.rm = T),
ret_lag2 = mean(abs(lag_retMkt2), na.rm = T),
ret_lag1 = mean(abs(lag_retMkt), na.rm = T),
ret1 = mean(abs(retMkt), na.rm = T),
lead_ret1 = mean(abs(lead_retMkt), na.rm = T),
lead_ret2 = mean(abs(lead_retMkt2), na.rm = T),
lead_ret3 = mean(abs(lead_retMkt3), na.rm = T),
lead_ret4 = mean(abs(lead_retMkt4), na.rm = T),
lead_ret5 = mean(abs(lead_retMkt5), na.rm = T)) %>%
t() %>%
as.data.frame() %>%
dplyr::rename(return = V1)
eventWindow <- data.frame(mean = eventWindow[12:22,1],
median = eventWindow[1:11,1])
eventWindow$time = c(-5,-4,-3,-2,-1,0,1,2,3,4,5)
p1 <- eventWindow %>%
pivot_longer(1:2, names_to = "measure") %>%
ggplot(aes(x = time, y = value, color = measure)) +
geom_point(data = . %>% filter(measure == "mean"),
size = 5, color = "blue", shape = 6) +
geom_line(data = . %>% filter(measure == "mean"),
color = "blue", linetype = "dotted", linewidth = 1, alpha = 0.7) +
geom_point(data = . %>% filter(measure == "median"),
size = 5, color = "green", shape = 4) +
geom_line(data = . %>% filter(measure == "median"),
color = "green", linetype = "dashed", linewidth = 1, alpha = 0.7) +
geom_vline(xintercept = 0, color = "darkgray", linetype = "dotted") +
ylab("Average/Median Absolute Excess Return") +
xlab("Time") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold")) +
labs(color = "Measure")
p1
ggsave("tables_and_plots/plots/p1.pdf", plot = p1, width = 7, height = 5)
rm(eventWindow, p1)
#-------------------------------------------------------------------------------
# DF data
#-------------------------------------------------------------------------------
df <- readRDS("fromR/df_meta.rds")
#-------------------------------------------------------------------------------
# Frequency graphs
#-------------------------------------------------------------------------------
# Frequency of year:
p <- meta %>%
group_by(year) %>%
summarise(nr_articles = n()) %>%
filter(year < 2023) %>%
ggplot(aes(x=year, y = nr_articles)) +
geom_bar(stat="identity", fill="blue", width=.8) +
xlab("\nYear") +
ylab("Number of press releases\n") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=9),
axis.title=element_text(size=12,face="bold")) +
scale_x_continuous(breaks = seq(min(meta$year), max(meta$year), 1), minor_breaks = NULL)
p
ggsave("tables_and_plots/plots/p10.pdf", plot = p, width = 7, height = 5)
# Frequency of tickers:
p <- meta %>%
group_by(ticker) %>%
summarise(nr_articles = n()) %>%
arrange(desc(nr_articles)) %>%
filter(nr_articles >= 100) %>%
mutate(ticker = fct_reorder(ticker, nr_articles)) %>%
ggplot( aes(x=ticker, y=nr_articles)) +
geom_bar(stat="identity", fill="blue", width=.4) +
coord_flip() +
xlab("") +
ylab("\nNumber of press releases") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=8),
axis.title=element_text(size=12,face="bold"))
p
ggsave("tables_and_plots/plots/p11.pdf", plot = p, width = 8, height = 12)
# Frequency of time of day:
meta$hr <- substr(meta$time_of_day, start = 1, stop = 2)
p <- meta %>%
group_by(hr) %>%
summarise(nr_articles = n()) %>%
ggplot(aes(x=hr, y = nr_articles)) +
geom_bar(stat="identity", fill="blue", width=.9) +
xlab("Hour of day") +
ylab("\nTotal number of press releases") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=8),
axis.title=element_text(size=12,face="bold"))
p
ggsave("tables_and_plots/plots/p12.pdf", plot = p, width = 7, height = 5)
colnames(meta)
# Industries:
meta$hr <- as.numeric(meta$hr)
colnames(meta)
p <- meta %>%
group_by(hr) %>%
summarise(
nr_articles = n(),
ret = median(ret)) %>%
ggplot(aes(x=hr, y=ret, color = "blue")) +
geom_line() +
guides(fill = "none")
p
meta <- readRDS("fromR/meta.rds")
p <- meta %>%
group_by(type) %>%
summarise(count = n()) %>%
select(-type) %>%
mutate(type = c("Regulated information required to be disclosed",
"Non-regulatory press releases",
"Inside information")) %>%
ggplot( aes(x=type, y=count)) +
geom_bar(stat="identity", fill="blue", width=.75) +
xlab("") +
ylab("\nNumber of press releases") +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=10),
axis.text.x = element_text(angle = 10, hjust = 1),
axis.title=element_text(size=12,face="bold"))
ggsave("tables_and_plots/plots/p13.pdf", plot = p, width = 7, height = 5)
meta$market <- substr(meta$market, 1, 4)
p <- meta %>%
group_by(market) %>%
summarise(count = n())
p[4,2] <- p[4,2] + p[2,2]
p <- p[-2, ]
p$market <- c("Euronext Growth", "Euronext Expand", "Oslo BÃ¸rs")
p
p <- meta %>%
group_by(market) %>%
summarise(count = n())
p
View(meta)
